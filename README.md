# ğŸ›’ AliExpress Web Scraper & Database Management System

Sistema completo de **web scraping** para monitorear la competencia en AliExpress y gestionar un catÃ¡logo de productos con anÃ¡lisis de precios, inventario y rentabilidad.

---

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n del Proyecto](#-descripciÃ³n-del-proyecto)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Base de Datos](#-base-de-datos)
- [CÃ³digo Python](#-cÃ³digo-python)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Uso](#-uso)
- [Consultas Ãštiles](#-consultas-Ãºtiles)
- [TecnologÃ­as Utilizadas](#-tecnologÃ­as-utilizadas)
- [Consideraciones de Seguridad](#-consideraciones-de-seguridad)

---

## ğŸ¯ DescripciÃ³n del Proyecto

Este proyecto permite a vendedores de AliExpress:

- **Automatizar** la recopilaciÃ³n de datos de productos de la competencia
- **Monitorear** precios, valoraciones, reseÃ±as y stock disponible
- **Gestionar** inventario propio y anÃ¡lisis de rentabilidad
- **Comparar** precios con la competencia
- **Generar alertas** sobre cambios importantes en el mercado

---

## ğŸ“ Estructura del Proyecto

```
PythonWebScraping/
â”‚
â”œâ”€â”€ example2.py                      # Script principal del scraper
â”œâ”€â”€ example.py                       # Script de prueba de conexiÃ³n
â”‚
â”œâ”€â”€ ScriptDB_Aliexpress.sql         # Dump completo de la BD (estructura + datos)
â”œâ”€â”€ InsertarDatosBD_Aliexpress.sql  # INSERTs de datos de ejemplo
â”œâ”€â”€ ConsultasBD_Aliexpress.sql      # Consultas SQL Ãºtiles para anÃ¡lisis
â”‚
â””â”€â”€ README.md                        # DocumentaciÃ³n (este archivo)
```

---

## ğŸ—„ï¸ Base de Datos

### Diagrama de Relaciones

La base de datos `aliexpress_vendedor` estÃ¡ diseÃ±ada con las siguientes tablas:

#### **Tabla: `Producto`**

CatÃ¡logo de productos propios del vendedor.

| Campo                 | Tipo          | DescripciÃ³n                        |
| --------------------- | ------------- | ---------------------------------- |
| `product_id`          | INT (PK)      | Identificador Ãºnico del producto   |
| `sku`                 | VARCHAR(50)   | CÃ³digo Ãºnico del producto (UNIQUE) |
| `nombre`              | VARCHAR(255)  | Nombre del producto                |
| `descripcion`         | TEXT          | DescripciÃ³n detallada              |
| `categoria`           | VARCHAR(100)  | CategorÃ­a del producto             |
| `precio_actual`       | DECIMAL(10,2) | Precio de venta actual             |
| `costo_proveedor`     | DECIMAL(10,2) | Costo de compra al proveedor       |
| `comision_aliexpress` | DECIMAL(5,2)  | % de comisiÃ³n de AliExpress        |
| `fecha_creacion`      | TIMESTAMP     | Fecha de creaciÃ³n del registro     |

#### **Tabla: `Datos_Competencia`** â­ (Principal para scraping)

Almacena informaciÃ³n de productos competidores extraÃ­da mediante web scraping.

| Campo                | Tipo          | DescripciÃ³n                        |
| -------------------- | ------------- | ---------------------------------- |
| `competencia_id`     | INT (PK)      | ID Ãºnico del registro              |
| `product_id`         | INT (FK)      | Referencia al producto propio      |
| `nombre_competidor`  | VARCHAR(255)  | Nombre del vendedor competidor     |
| `precio_competencia` | DECIMAL(10,2) | Precio del competidor              |
| `stock_visible`      | TINYINT(1)    | Stock visible (1=disponible, 0=no) |
| `valoracion`         | DECIMAL(3,2)  | ValoraciÃ³n promedio (1-5)          |
| `numero_resenas`     | INT           | NÃºmero de reseÃ±as                  |
| `url`                | VARCHAR(500)  | URL del producto competidor        |
| `fecha_scraping`     | TIMESTAMP     | Fecha y hora del scraping          |

#### **Tabla: `Inventario`**

Control de stock de productos propios.

| Campo                 | Tipo      | DescripciÃ³n                         |
| --------------------- | --------- | ----------------------------------- |
| `inventario_id`       | INT (PK)  | ID Ãºnico                            |
| `product_id`          | INT (FK)  | Referencia al producto              |
| `stock_actual`        | INT       | Cantidad disponible actualmente     |
| `stock_minimo`        | INT       | Stock mÃ­nimo antes de reordenar     |
| `stock_maximo`        | INT       | Stock mÃ¡ximo permitido              |
| `fecha_actualizacion` | TIMESTAMP | Ãšltima actualizaciÃ³n del inventario |

#### **Tabla: `Venta`**

Registro histÃ³rico de ventas.

| Campo             | Tipo          | DescripciÃ³n             |
| ----------------- | ------------- | ----------------------- |
| `venta_id`        | INT (PK)      | ID Ãºnico de la venta    |
| `product_id`      | INT (FK)      | Producto vendido        |
| `cantidad`        | INT           | Unidades vendidas       |
| `precio_unitario` | DECIMAL(10,2) | Precio por unidad       |
| `precio_total`    | DECIMAL(10,2) | Total de la venta       |
| `fecha_venta`     | TIMESTAMP     | Fecha de la transacciÃ³n |

#### **Otras Tablas**

- **`Proveedor`**: InformaciÃ³n de proveedores
- **`Producto_Proveedor`**: RelaciÃ³n muchos a muchos entre productos y proveedores
- **`Valoracion`**: ReseÃ±as de clientes sobre productos propios
- **`Alerta`**: Sistema de notificaciones automÃ¡ticas

### Relaciones Clave

- `Producto` â†’ `Datos_Competencia` (1:N)
- `Producto` â†’ `Inventario` (1:1)
- `Producto` â†’ `Venta` (1:N)
- `Producto` â†’ `Valoracion` (1:N)

---

## ğŸ CÃ³digo Python

### Clase Principal: `AliExpressScraper`

#### **Constructor**

```python
def __init__(self, db_host, db_user, db_password, db_name):
```

Inicializa la conexiÃ³n a MySQL y configura headers para las peticiones HTTP.

**ParÃ¡metros:**

- `db_host`: Servidor MySQL (ej: "localhost")
- `db_user`: Usuario de MySQL (ej: "root")
- `db_password`: ContraseÃ±a de MySQL
- `db_name`: Nombre de la base de datos (ej: "aliexpress_vendedor")

#### **MÃ©todo: `extraer_producto(url)`**

```python
def extraer_producto(self, url):
```

Extrae informaciÃ³n de un producto de AliExpress mediante web scraping.

**Retorna un diccionario con:**

- `nombre`: Nombre del producto
- `precio`: Precio en formato decimal
- `valoracion`: CalificaciÃ³n promedio
- `numero_resenas`: Cantidad de reseÃ±as
- `stock_visible`: Disponibilidad de stock
- `url`: URL original

**Manejo de errores:**

- Utiliza `try/except` para capturar errores HTTP
- Logging de errores con `logger.error()`
- Retorna `None` si falla la extracciÃ³n

#### **MÃ©todo: `insertar_en_bd(producto, product_id)`**

```python
def insertar_en_bd(self, producto, product_id):
```

Inserta los datos extraÃ­dos en la tabla `Datos_Competencia`.

**ParÃ¡metros:**

- `producto`: Diccionario con datos del producto
- `product_id`: ID del producto propio con el que se relaciona

**Query SQL:**

```sql
INSERT INTO Datos_Competencia
(product_id, nombre_competidor, precio_competencia,
 stock_visible, valoracion, numero_resenas, url)
VALUES (%s, %s, %s, %s, %s, %s, %s)
```

#### **MÃ©todo: `ejecutar_scraping(urls)`**

```python
def ejecutar_scraping(self, urls):
```

Itera sobre una lista de URLs y ejecuta el scraping con intervalo de 3 segundos entre peticiones (para evitar bloqueos).

### Flujo de EjecuciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Inicializar    â”‚
â”‚    Scraper      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conectar a BD  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Iterar URLs   â”‚â—„â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
         â”‚                â”‚
         â–¼                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚ HTTP Request    â”‚       â”‚
â”‚ (BeautifulSoup) â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
         â”‚                â”‚
         â–¼                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  Parsear HTML   â”‚       â”‚
â”‚  Extraer Datos  â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
         â”‚                â”‚
         â–¼                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   INSERT BD     â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
         â”‚                â”‚
         â–¼                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  Sleep 3 seg    â”‚â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ InstalaciÃ³n

### 1. **Requisitos Previos**

- Python 3.7+
- MySQL 8.0+
- pip (gestor de paquetes de Python)

### 2. **Clonar el Repositorio**

```bash
git clone https://github.com/Emjey25/JavandoCodigo.git
cd PythonWebScraping
```

### 3. **Instalar Dependencias Python**

```powershell
pip install requests beautifulsoup4 mysql-connector-python
```

**Dependencias:**

- `requests`: Para hacer peticiones HTTP
- `beautifulsoup4`: Para parsear HTML
- `mysql-connector-python`: Conector MySQL

### 4. **Configurar Base de Datos**

#### OpciÃ³n A: Importar dump completo

```bash
mysql -u root -p < ScriptDB_Aliexpress.sql
```

#### OpciÃ³n B: Crear manualmente

```bash
# 1. Crear la base de datos
mysql -u root -p -e "CREATE DATABASE aliexpress_vendedor;"

# 2. Importar estructura
mysql -u root -p aliexpress_vendedor < ScriptDB_Aliexpress.sql

# 3. Insertar datos de ejemplo (opcional)
mysql -u root -p aliexpress_vendedor < InsertarDatosBD_Aliexpress.sql
```

### 5. **Configurar Credenciales**

âš ï¸ **IMPORTANTE**: Edita `example2.py` y reemplaza las credenciales:

```python
scraper = AliExpressScraper(
    'localhost',           # db_host
    'root',                # db_user
    'TU_CONTRASEÃ‘A_AQUI', # db_password (NO dejar en el cÃ³digo)
    'aliexpress_vendedor'  # db_name
)
```

**RecomendaciÃ³n de seguridad**: Usa variables de entorno:

```python
import os
password = os.getenv('MYSQL_PASSWORD')
```

---

## ğŸ’» Uso

### EjecuciÃ³n BÃ¡sica

```powershell
python example2.py
```

### Personalizar URLs

Edita la lista de URLs en `example2.py`:

```python
urls = [
    'https://es.aliexpress.com/item/1005007813235975.html',
    'https://es.aliexpress.com/item/OTRO_PRODUCTO.html',
    # Agrega mÃ¡s URLs aquÃ­
]
```

### Ajustar Product ID

El scraper asocia cada producto competidor a un `product_id` de tu catÃ¡logo. Modifica segÃºn corresponda:

```python
# En el mÃ©todo ejecutar_scraping:
self.insertar_en_bd(producto, 1)  # Cambiar '1' por el ID real
```

### Salida Esperada

```
INFO:__main__:Scrapeando: https://es.aliexpress.com/item/...
INFO:__main__:ExtraÃ­do: Protector Pantalla Vidrio Templado
INFO:__main__:Insertado en BD
```

---

## ğŸ“Š Consultas Ãštiles

### 1. Productos con Stock Bajo

```sql
SELECT p.nombre, i.stock_actual, i.stock_minimo
FROM Producto p
JOIN Inventario i ON p.product_id = i.product_id
WHERE i.stock_actual < i.stock_minimo;
```

### 2. Productos MÃ¡s Vendidos (Ãºltimos 30 dÃ­as)

```sql
SELECT p.nombre, SUM(v.cantidad) as total
FROM Producto p
JOIN Venta v ON p.product_id = v.product_id
WHERE v.fecha_venta >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)
GROUP BY p.product_id
ORDER BY total DESC;
```

### 3. ComparaciÃ³n de Precios con Competencia

```sql
SELECT
    p.nombre,
    p.precio_actual,
    dc.precio_competencia,
    ROUND(((p.precio_actual - dc.precio_competencia) / dc.precio_competencia * 100), 2)
        as diferencia_porcentual
FROM Producto p
JOIN Datos_Competencia dc ON p.product_id = dc.product_id;
```

### 4. AnÃ¡lisis de Rentabilidad

```sql
SELECT
    p.nombre,
    p.precio_actual,
    p.costo_proveedor,
    (p.precio_actual - p.costo_proveedor -
     (p.precio_actual * p.comision_aliexpress / 100)) as margen
FROM Producto p
ORDER BY margen DESC;
```

### 5. Historial de Precios de Competencia

```sql
SELECT
    dc.nombre_competidor,
    dc.precio_competencia,
    dc.fecha_scraping
FROM Datos_Competencia dc
WHERE dc.product_id = 1
ORDER BY dc.fecha_scraping DESC
LIMIT 10;
```

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

| TecnologÃ­a      | VersiÃ³n | PropÃ³sito                |
| --------------- | ------- | ------------------------ |
| Python          | 3.7+    | Lenguaje principal       |
| MySQL           | 8.0+    | Base de datos relacional |
| BeautifulSoup4  | 4.x     | Parseo de HTML           |
| Requests        | 2.x     | Peticiones HTTP          |
| mysql-connector | 8.x     | Conector Python-MySQL    |
| Git             | 2.x     | Control de versiones     |

---

## ğŸ”’ Consideraciones de Seguridad

### 1. **Credenciales**

âŒ **NO** guardes contraseÃ±as en el cÃ³digo
âœ… Usa variables de entorno o archivos `.env`

```python
# Crear archivo .env:
# MYSQL_PASSWORD=tu_contraseÃ±a

import os
from dotenv import load_dotenv

load_dotenv()
password = os.getenv('MYSQL_PASSWORD')
```

### 2. **Web Scraping Responsable**

- â±ï¸ Respetar intervalos de tiempo entre peticiones (actualmente 3 segundos)
- ğŸ¤– Identificarte con User-Agent apropiado
- ğŸ“œ Revisar los TÃ©rminos de Servicio de AliExpress
- ğŸš« No sobrecargar servidores con peticiones masivas

### 3. **Manejo de Errores**

El cÃ³digo incluye logging de errores pero considera:

- Implementar reintentos con exponential backoff
- Guardar logs en archivos para auditorÃ­a
- Monitorear cambios en la estructura HTML de AliExpress

### 4. **InyecciÃ³n SQL**

âœ… El cÃ³digo usa consultas parametrizadas (`%s`) lo cual previene inyecciÃ³n SQL:

```python
cursor.execute(query, datos)  # âœ… Correcto
# cursor.execute(f"INSERT... {variable}")  # âŒ NUNCA hacer esto
```

---

## ğŸ“ Notas Adicionales

### Limitaciones Actuales

- **JavaScript**: BeautifulSoup solo parsea HTML estÃ¡tico. Si AliExpress carga datos con JavaScript, considera usar:

  - Selenium
  - Playwright
  - Puppeteer
  - API pÃºblica (si existe)

- **Selectores**: Los selectores CSS pueden cambiar. El cÃ³digo usa clases genÃ©ricas como fallback.

- **Captchas**: AliExpress puede bloquear scraping con captchas. Soluciones:
  - Proxies rotatorios
  - Servicios de resoluciÃ³n de captchas
  - Reducir frecuencia de peticiones

### Mejoras Futuras

- [ ] Implementar scraping asÃ­ncrono (aiohttp)
- [ ] Dashboard web con Flask/Django
- [ ] Notificaciones automÃ¡ticas por email
- [ ] Soporte para mÃºltiples marketplaces
- [ ] Sistema de cachÃ© para URLs ya scrapeadas
- [ ] Tests unitarios con pytest
- [ ] DockerizaciÃ³n del proyecto

---

## ğŸ‘¨â€ğŸ’» Autor

**Emjey25**  
GitHub: [@Emjey25](https://github.com/Emjey25)

---

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto. Ãšsalo bajo tu propia responsabilidad y respeta las polÃ­ticas de web scraping de los sitios objetivo.

---

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Si encuentras bugs o tienes mejoras:

1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/nueva-caracteristica`)
3. Commit cambios (`git commit -m 'Agregar nueva caracterÃ­stica'`)
4. Push a la rama (`git push origin feature/nueva-caracteristica`)
5. Abre un Pull Request

---

## âš ï¸ Disclaimer

Este proyecto es **solo con fines educativos**. El web scraping puede violar los TÃ©rminos de Servicio de algunos sitios web. Ãšsalo de manera responsable y Ã©tica. El autor no se hace responsable del uso indebido de este cÃ³digo.

---

**Ãšltima actualizaciÃ³n**: Noviembre 2025  
**VersiÃ³n**: 1.0.0
